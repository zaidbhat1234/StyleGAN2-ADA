{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "StyleGAN2-ada.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP1z475+97/Xx7cmwt94pGN",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zaidbhat1234/StyleGAN2-ADA/blob/main/StyleGAN2_ada.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-pGT7quxk-B"
      },
      "source": [
        "##Mounting your google drive containing the code files.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uY8nqGfg6L8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd /content/gdrive/My\\ Drive/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xZa-pCpxzKd"
      },
      "source": [
        "##The code should be arranged in this order of directories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kycNZh5HxDWL"
      },
      "source": [
        "%cd KAUST_Internship/stylegan2-ada/stylegan2-pytorch/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wV7Rb45Iyg5_"
      },
      "source": [
        "##Resolving dependencies and importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPC8g7ItvrZL"
      },
      "source": [
        "!wget https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
        "!sudo unzip ninja-linux.zip -d /usr/local/bin/\n",
        "!sudo update-alternatives --install /usr/bin/ninja ninja /usr/local/bin/ninja 1 --force \n",
        "import argparse\n",
        "import torch\n",
        "from torchvision import utils\n",
        "from model import Generator\n",
        "from tqdm import tqdm\n",
        "import lpips\n",
        "import math\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from collections import OrderedDict\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import torchvision\n",
        "from torchvision import models\n",
        "from torchvision.utils import save_image\n",
        "import numpy as np\n",
        "from math import log10\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaRLCMxAyoYI"
      },
      "source": [
        "##Setup for running on GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sL9prZCadIfA"
      },
      "source": [
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0qCvzz1ywlY"
      },
      "source": [
        "##Function to read some images in a loop for testing, where 'i' is the index of the image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k46lPtW6EGSP"
      },
      "source": [
        "def read_img(i):\n",
        "  img_path = '/content/gdrive/My Drive/KAUST_Internship/StyleGAN_LatentEditor/images/'+i+'.png'\n",
        "  with open(img_path,\"rb\") as f: \n",
        "    image=Image.open(f)\n",
        "    image=image.convert(\"RGB\")\n",
        "  transform = transforms.Compose([transforms.ToTensor()])\n",
        "  image = transform(image)\n",
        "  image = image.unsqueeze(0)\n",
        "  image = image.to(device)\n",
        "  print(image.shape)\n",
        "  return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88cD6Aa4zfn_"
      },
      "source": [
        "##Load the StyleGan2 generator from pre-trained weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-0NX28GnTyp"
      },
      "source": [
        "g_ema = Generator(1024, 512, 8)\n",
        "checkpoint = torch.load('ffhq2.pt')\n",
        "g_ema.load_state_dict(checkpoint[\"g_ema\"])\n",
        "g_ema.eval()\n",
        "g_ema = g_ema.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNUOPVMzzook"
      },
      "source": [
        "##Get the mean latent code from the SG2 network and generate the corresponding image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BReNtY0RMSGL"
      },
      "source": [
        "mean_latent = g_ema.mean_latent(4096)\n",
        "print(mean_latent.shape)\n",
        "img,_ = g_ema([mean_latent])\n",
        "img = (img +1.0)/2.0\n",
        "save_image(img.clamp(0,1),\"outputs/mean_latent.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEIaiYiKz2o9"
      },
      "source": [
        "##Randomly generate images corresponding to random latent codes 'w'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evbdtTvzJ6Lt"
      },
      "source": [
        "\"\"\"\n",
        "for i in range(20):\n",
        "  z = torch.randn(1,512,device = device)\n",
        "  \n",
        "  img,_ = g_ema([z])\n",
        "  img = (img +1.0)/2.0\n",
        "  save_image(img.clamp(0,1),\"outputs/random_SG2-{}.png\".format(i+1))\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_WIvEXsfzRr"
      },
      "source": [
        "\"\"\"n_mean_latent = 10000\n",
        "noise_sample = torch.randn(n_mean_latent, 512, device=device)\n",
        "latent_out = g_ema.style(noise_sample)\n",
        "\n",
        "latent_mean = latent_out.mean(0)\n",
        "latent_std = ((latent_out - latent_mean).pow(2).sum() / n_mean_latent) ** 0.5\n",
        "latent_in = latent_mean.detach().clone().unsqueeze(0).repeat(1, 1)\n",
        "latent_in = latent_in.unsqueeze(1).repeat(1, g_ema.n_latent, 1)\n",
        "print(np.shape(latent_in))\n",
        "latents = torch.zeros((1,18,512))\n",
        "img_gen, _ = g_ema([latents], input_is_latent=True)\n",
        "img_gen = (img_gen+1.0)/2.0\n",
        "save_image(img_gen,\"gen.png\")\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aD3JrfO91P_3"
      },
      "source": [
        "##VGG Perceptual loss network to give feature vectors from 4 parts of the pre-trained VGG-16 from 2,4,14,21"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxVYTQ9IrpqA"
      },
      "source": [
        "class VGG16_perceptual(torch.nn.Module):\n",
        "    def __init__(self, requires_grad=False):\n",
        "        super(VGG16_perceptual, self).__init__()\n",
        "        vgg_pretrained_features = models.vgg16(pretrained=True).features\n",
        "        self.slice1 = torch.nn.Sequential()\n",
        "        self.slice2 = torch.nn.Sequential()\n",
        "        self.slice3 = torch.nn.Sequential()\n",
        "        self.slice4 = torch.nn.Sequential()\n",
        "        for x in range(2):\n",
        "            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(2, 4):\n",
        "            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(4, 14):\n",
        "            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(14, 21):\n",
        "            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n",
        "        if not requires_grad:\n",
        "            for param in self.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, X):\n",
        "        h = self.slice1(X)\n",
        "        h_relu1_1 = h\n",
        "        h = self.slice2(h)\n",
        "        h_relu1_2 = h\n",
        "        h = self.slice3(h)\n",
        "        h_relu3_2 = h\n",
        "        h = self.slice4(h)\n",
        "        h_relu4_2 = h\n",
        "        return h_relu1_1, h_relu1_2, h_relu3_2, h_relu4_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjKhmBBa1bE5"
      },
      "source": [
        "##Loss function to calculate MSE and Perceptual losses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guPF2IBKhRr3"
      },
      "source": [
        "def loss_function(syn_img, img, img_p, MSE_loss, upsample, perceptual):\n",
        "\n",
        "  #UpSample synthesized image to match the input size of VGG-16 input. \n",
        "  #Extract mid level features for real and synthesized image and find the MSE loss between them for perceptual loss. \n",
        "  #Find MSE loss between the real and synthesized images of actual size\n",
        "  syn_img_p = upsample(syn_img)\n",
        "  syn0, syn1, syn2, syn3 = perceptual(syn_img_p)\n",
        "  r0, r1, r2, r3 = perceptual(img_p)\n",
        "  mse = MSE_loss(syn_img,img)\n",
        "\n",
        "  per_loss = 0\n",
        "  per_loss += MSE_loss(syn0,r0)\n",
        "  per_loss += MSE_loss(syn1,r1)\n",
        "  per_loss += MSE_loss(syn2,r2)\n",
        "  per_loss += MSE_loss(syn3,r3)\n",
        "\n",
        "  return mse, per_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MV8QHv-P1qQ0"
      },
      "source": [
        "##Calculate PSNR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_9i01OUhUH9"
      },
      "source": [
        "def PSNR(mse, flag = 0):\n",
        "  #flag = 0 if a single image is used and 1 if loss for a batch of images is to be calculated\n",
        "\n",
        "  if flag == 0:\n",
        "    psnr = 10 * log10(1 / mse.item())\n",
        "  \n",
        "  return psnr\n",
        "psnr_total = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpoKbmvE2DdJ"
      },
      "source": [
        "##Noise Regulariser"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSpTdIlSgQ2O"
      },
      "source": [
        "def noise_regularize(noises):\n",
        "    loss = 0\n",
        "    for noise in noises:\n",
        "        size = noise.shape[2]\n",
        "        while True:\n",
        "            loss = (\n",
        "                loss\n",
        "                + (noise * torch.roll(noise, shifts=1, dims=3)).mean().pow(2)\n",
        "                + (noise * torch.roll(noise, shifts=1, dims=2)).mean().pow(2)\n",
        "            )\n",
        "            if size <= 8:\n",
        "                break\n",
        "            noise = noise.reshape([-1, 1, size // 2, 2, size // 2, 2])\n",
        "            noise = noise.mean([3, 5])\n",
        "            size //= 2\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjBB1pwIEEEh"
      },
      "source": [
        "def latent_noise(latent, strength):\n",
        "    noise = torch.randn_like(latent) * strength\n",
        "    return latent + noise"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Wt-y5eJHWv_"
      },
      "source": [
        "def get_lr(t, initial_lr, rampdown=0.25, rampup=0.05):\n",
        "    lr_ramp = min(1, (1 - t) / rampdown)\n",
        "    lr_ramp = 0.5 - 0.5 * math.cos(lr_ramp * math.pi)\n",
        "    lr_ramp = lr_ramp * min(1, t / rampup)\n",
        "\n",
        "    return initial_lr * lr_ramp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7_EjFpf2Nc3"
      },
      "source": [
        "##Initial value of Noise to be optimised"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b23EyyzPcy1C"
      },
      "source": [
        "def init_noise():\n",
        "  noises_single = g_ema.make_noise()\n",
        "  noises = []\n",
        "  for noise in noises_single:\n",
        "    noises.append(noise.repeat(1, 1, 1, 1).normal_())\n",
        "  for noise in noises:\n",
        "    noise.requires_grad = True\n",
        "  print(len(noises))\n",
        "  return noises"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4O1xbb3f9F4O"
      },
      "source": [
        "latent_out = g_ema.style(mean_latent)\n",
        "latent_mean = latent_out.mean(0)\n",
        "latent_std = ((latent_out - latent_mean).pow(2).sum() / 10000) ** 0.5\n",
        "latent_in = latent_mean.detach().clone().unsqueeze(0).repeat(1, 1)\n",
        "latent_in = latent_in.unsqueeze(1).repeat(1, g_ema.n_latent, 1)\n",
        "latent_in.requires_grad = True\n",
        "latents = latent_in\n",
        "latents.requires_grad = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_URlhua3JM4"
      },
      "source": [
        "##Initialising latent vector W+ from mean latent W"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1qMDtZXYgK6"
      },
      "source": [
        "#Mean latent w+\n",
        "def initialise():\n",
        "  latent_out = g_ema.style(mean_latent)\n",
        "  latent_mean = latent_out.mean(0)\n",
        "  latent_std = ((latent_out - latent_mean).pow(2).sum() / 10000) ** 0.5\n",
        "  latent_in = latent_mean.detach().clone().unsqueeze(0).repeat(1, 1)\n",
        "  latent_in = latent_in.unsqueeze(1).repeat(1, g_ema.n_latent, 1)\n",
        "  latent_in.requires_grad = True\n",
        "  latents = latent_in\n",
        "  latents.requires_grad = True\n",
        "  #print((latent_in))\n",
        "  return latents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mK7lBcu3TRL"
      },
      "source": [
        "##Function to Penalise successive W+ codes to be similar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PTSMsT7QJtA"
      },
      "source": [
        "def penalise_w(latent):\n",
        "  loss = 0\n",
        "  MSE_loss = nn.MSELoss(reduction=\"mean\")\n",
        "  for i in range(17):\n",
        "    loss += MSE_loss(latent[:,i+1,:],latent[:,i,:])\n",
        "  #loss = loss/18\n",
        "  #print(loss)\n",
        "  return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiSOioEK3a7b"
      },
      "source": [
        "##Computer Average W for hacky smoothing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeXcxaCt3CbI"
      },
      "source": [
        "def average_w(latent1):\n",
        "  #print(latent[:,1,:].shape)\n",
        "  #Check if this is correct\n",
        "  #latent1.requires_grad = False\n",
        "  mean = torch.mean(latent1, dim=1)\n",
        "  lambdaa = 0.2\n",
        "  #print(mean.shape)\n",
        "  latent = latent1\n",
        "  for i in range(17):\n",
        "    latent[:,i,:] = lambdaa * latent1[:,i,:] + (1.0-lambdaa) * mean\n",
        "  #print(latent.shape)\n",
        "  #latent1.requires_grad = True\n",
        "  return latent\n",
        "#average_w(latents)\n",
        "\"\"\"\n",
        " if (e+1)%50 ==0:\n",
        "      #Hacky Smoothing\n",
        "      print(\"hacky smoothing\")\n",
        "      latents.requires_grad = False\n",
        "      latents = average_w(latents)\n",
        "      latents.requires_grad = True\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7Vd6tu645zn"
      },
      "source": [
        "###Masking the image for an experiment which required calculating loss on the centre cropped image rather than the downsampled image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8bW6rc-hHoS"
      },
      "source": [
        "mask = torch.ones(256,256)\n",
        "zero = torch.zeros(1024,1024)\n",
        "zero[384:640,384:640] = mask \n",
        "mask = zero\n",
        "mask = mask.unsqueeze(0)\n",
        "mask = torch.cat(3*[mask])\n",
        "mask = mask.unsqueeze(0)\n",
        "print(mask.shape)\n",
        "mask = mask.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTX003E25HDq"
      },
      "source": [
        "##Embedding Function to optimise the latent code W+ for GAN inversion."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQy0TgNfav-A"
      },
      "source": [
        "def embedding_function(image,latent,x):\n",
        "  upsample = torch.nn.Upsample(scale_factor = 256/1024, mode = 'bilinear')\n",
        "  img_p = image.clone()\n",
        "  img_p = upsample(img_p) #Downsample image to 256x256 for \n",
        "  img_c = image.clone()\n",
        "  img_c = img_c * mask #Centre crop image with mask\n",
        "  img_c_p = upsample(img_c)\n",
        "\n",
        "  #Initialise VGG-perceptual loss\n",
        "  perceptual1 = VGG16_perceptual().to(device)\n",
        "  #Initialise LPIPS-perceptual loss\n",
        "  #perceptual = lpips.PerceptualLoss(model=\"net-lin\", net=\"vgg\")\n",
        "  \n",
        "  #MSE loss object\n",
        "  MSE_loss = nn.MSELoss(reduction=\"mean\")\n",
        "\n",
        "  #latents = torch.zeros((1,18,512), requires_grad = True, device = device)\n",
        "  latents = latent\n",
        "  latents.requires_grad = True\n",
        "  latents.to(device)\n",
        "  \n",
        "  #Optimizer to change latent code in each backward step\n",
        "  optimizer = optim.Adam({latents},lr=0.01) #,betas=(0.9,0.999),eps=1e-8 [latents]+noises\n",
        "\n",
        "\n",
        "  #Loop to optimise latent vector to match the generated image to input image\n",
        "  loss_ = []\n",
        "  loss_psnr = []\n",
        "  final_psnr = 0\n",
        "  for e in range(1500):\n",
        "    optimizer.zero_grad()\n",
        "    syn_img,_ = g_ema([latents], input_is_latent=True)\n",
        "    syn_img = (syn_img+1.0)/2.0\n",
        "    mse, per_loss = loss_function(syn_img, image, img_p, MSE_loss, upsample, perceptual1)\n",
        "    psnr = PSNR(mse, flag = 0)\n",
        "\n",
        "    #n_loss = noise_regularize(noises)\n",
        "    #per_loss = perceptual(syn_img, image).sum()\n",
        "    #loss_w = penalise_w(latents) \n",
        "\n",
        "    # To use centre crop and downsampled for LPIPS\n",
        "    #syn_img_p = syn_img.clone()\n",
        "    #syn_img_p = upsample(syn_img)\n",
        "    #syn_img_c = syn_img.clone()\n",
        "    #syn_img_c = syn_img_c * mask\n",
        "    #print(syn_img_p.shape,syn_img_c.shape)\n",
        "    #per_loss_down = perceptual(syn_img_p, img_p).sum()\n",
        "    #per_loss_crop = perceptual(syn_img_c, img_c).sum()\n",
        "    #loss_pd = per_loss_down.detach().cpu().numpy()\n",
        "    #loss_cd = per_loss_crop.detach().cpu().numpy()\n",
        "    #mse1, per_loss1 = loss_function(syn_img_c,  img_c, img_c_p, MSE_loss, upsample, perceptual1)\n",
        "    \n",
        "    #loss is sum of losses of downsampled and centre cropeed image\n",
        "    #per_loss = per_loss +per_loss1 \n",
        "    #mse = mse+mse1\n",
        "\n",
        "    loss = per_loss +mse \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    loss_np=loss.detach().cpu().numpy()\n",
        "    loss_p=per_loss.detach().cpu().numpy()\n",
        "    loss_m=mse.detach().cpu().numpy()\n",
        "    loss_psnr.append(psnr)\n",
        "    loss_.append(loss_np)\n",
        "    final_psnr = psnr\n",
        "    \n",
        "    if (e+1)%500==0:\n",
        "      print(\"iter{}: loss -- {},  mse_loss --{},  percep_loss --{}, psnr --{}\".format(e+1,loss_np,loss_m,loss_p,psnr))\n",
        "      #print(\"iter{}: loss--{},mse_loss--{},per_loss_down--{},per_loss_crop--{},psnr--{}\".format(e+1,loss_np,loss_m,loss_pd,loss_cd, psnr))\n",
        "      save_image(syn_img.clamp(0,1),\"outputs/Step1-2VGG-{}-{}.png\".format(x,e+1)) #Save Images\n",
        "      #np.save(\"loss_list.npy\",loss_)\n",
        "      #np.save(\"latent_W.npy\".format(),latents.detach().cpu().numpy())\n",
        "\n",
        "  plt.plot(loss_, label = 'Loss = MSELoss + Perceptual')\n",
        "  plt.plot(loss_psnr, label = 'PSNR')\n",
        "  plt.legend()\n",
        "  return latents, final_psnr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75_Qqpgyh2CI"
      },
      "source": [
        "latents, psnr = embedding_function(image,latents,x) #Calling the embedding function to optimise latent W. It returns the optimised latent code which can be further optimised for other experiments"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHAYIVTn8Vkm"
      },
      "source": [
        "##Embedding function to optimise W+ along with noise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5GeQvnXEQL2"
      },
      "source": [
        "def embedding_function_n(image,latent,x):\n",
        "  upsample = torch.nn.Upsample(scale_factor = 256/1024, mode = 'bilinear')\n",
        "  img_p = image.clone()\n",
        "  img_p = upsample(img_p)\n",
        "  #print(img_p.shape)\n",
        "\n",
        "  #Initialise perceptual losses initialise object\n",
        "  perceptual1 = VGG16_perceptual().to(device)\n",
        "  perceptual = lpips.PerceptualLoss(model=\"net-lin\", net=\"vgg\")\n",
        "  \n",
        "  #MSE loss object\n",
        "  MSE_loss = nn.MSELoss(reduction=\"mean\")\n",
        "\n",
        "  latents = latent\n",
        "  latents.requires_grad = True\n",
        "  latents.to(device)\n",
        "\n",
        "  #Optimizer to change latent code in each backward step\n",
        "  optimizer = optim.Adam([latents]+noises,lr=0.1) #,betas=(0.9,0.999),eps=1e-8 [latents]+noises\n",
        "\n",
        "  pbar = tqdm(range(1000))\n",
        "\n",
        "  #Loop to optimise latent vector to match the generated image to input image\n",
        "  loss_ = []\n",
        "  loss_psnr = []\n",
        "  final_psnr = 0\n",
        "  for e in pbar:\n",
        "    t = e / 1000\n",
        "    lr = get_lr(t, 0.1)\n",
        "    optimizer.param_groups[0][\"lr\"] = lr\n",
        "    noise_strength = latent_std * 0.05 * max(0, 1 - t / 0.75) ** 2\n",
        "    latent_n = latent_noise(latents, noise_strength.item())\n",
        "    \n",
        "    syn_img,_ = g_ema([latent_n], input_is_latent=True, noise= noises)\n",
        "    syn_img = (syn_img+1.0)/2.0\n",
        "    syn_img_p = upsample(syn_img)\n",
        "    mse, per_loss1 = loss_function(syn_img, image, img_p, MSE_loss, upsample, perceptual1)\n",
        "    psnr = PSNR(mse, flag = 0)\n",
        "    final_psnr = psnr\n",
        "    n_loss = noise_regularize(noises)\n",
        "    per_loss = perceptual(syn_img_p, img_p).sum()\n",
        "    #loss_w = penalise_w(latents) \n",
        "    loss = per_loss + mse +1e5 * n_loss\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    loss_np=loss.detach().cpu().numpy()\n",
        "    loss_p=per_loss.detach().cpu().numpy()\n",
        "    loss_m=mse.detach().cpu().numpy()\n",
        "    loss_psnr.append(psnr)\n",
        "    loss_.append(loss_np)\n",
        "    if (e+1)%1000==0:\n",
        "      #print(\"penalise:\", loss_w)\n",
        "      print(\"iter{}: loss -- {},  mse_loss --{},  percep_loss --{}, psnr --{}\".format(e+1,loss_np,loss_m,loss_p,psnr))\n",
        "      save_image(syn_img.clamp(0,1),\"outputs/Step2-LPIPS-256 scratch-{}-{}.png\".format(x,e+1))\n",
        "      #np.save(\"loss_list.npy\",loss_)\n",
        "      #np.save(\"latent_W.npy\".format(),latents.detach().cpu().numpy())\n",
        "\n",
        "  plt.plot(loss_, label = 'Loss = MSELoss + Perceptual')\n",
        "  plt.plot(loss_psnr, label = 'PSNR')\n",
        "  plt.legend()\n",
        "  return latents, final_psnr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svD6rnW7GwyZ"
      },
      "source": [
        "latents,psnr = embedding_function_n(image,latents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLmccA8w-3tc"
      },
      "source": [
        "##Loop to iterate over 10 images to find the average PSNR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-1KM8D12mSn"
      },
      "source": [
        "t_psnr=[]\n",
        "t_psnr_c=[]\n",
        "t_psnr_s = []\n",
        "for i in range(10):\n",
        "  image = read_img(str(i+1))\n",
        "  latents = initialise()\n",
        "  noises = init_noise()\n",
        "  latents, psnr = embedding_function(image, latents,i+1) #only vgg\n",
        "  #latents, psnr1 = embedding_function_n(image, latents,i+1) #LPIPS continued\n",
        "  #t_psnr.append(psnr)\n",
        "  #t_psnr_c.append(psnr1)\n",
        "  t_psnr_s.append(psnr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRx1WfOvuE3l"
      },
      "source": [
        "print('average psnr vgg', t_psnr)\n",
        "print('average psnr lpips continue', t_psnr_c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xfIFWLSbBEg"
      },
      "source": [
        "print('average psnr 2lpips', t_psnr_s)\n",
        "av1 = np.sum(t_psnr_s)/10\n",
        "print('average',av1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dq4f1K_0Sn9T"
      },
      "source": [
        "av = np.sum(t_psnr)/10\n",
        "av1 = np.sum(t_psnr_c)/10\n",
        "print('average',av,av1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0SLlS5F_em2"
      },
      "source": [
        "##Incomplete code in TensorFlow before switching to Pytorch\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EisayNXM2rUz"
      },
      "source": [
        "!python projector.py --ckpt ffhq2.pt --size 1024 '/content/gdrive/My Drive/KAUST_Internship/StyleGAN_LatentEditor/images/ryan_01.png'\n",
        "#Official TF version\n",
        "!pip install tensorflow==1.14\n",
        "!pip install tensorflow-gpu==1.14\n",
        "import argparse\n",
        "import os\n",
        "import pickle\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "\n",
        "import dnnlib\n",
        "import dnnlib.tflib as tflib\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "from torchvision import models\n",
        "tflib.init_tf()\n",
        "network_pkl = 'ffhq.pkl'\n",
        "print('Loading networks from \"%s\"...' % network_pkl)\n",
        "with dnnlib.util.open_url(network_pkl) as fp:\n",
        "  _G, _D, Gs = pickle.load(fp)\n",
        "# load Image\n",
        "img = PIL.Image.open('images/img1.png').convert('RGB')\n",
        "img = np.array(img, dtype=np.uint8)\n",
        "img = img.astype(np.float32).transpose([2, 0, 1]) * (2 / 255) - 1\n",
        "print(img.shape)\n",
        "\n",
        "# Downsample image to 256x256 if it's larger than that. VGG was built for 224x224 images.\n",
        "def downsample(image):\n",
        "  img_sample = tf.expand_dims(img,axis=0)\n",
        "  img_sample = (img_sample + 1) * (255 / 2)\n",
        "  sh = img_sample.shape.as_list()\n",
        "  if sh[2] > 256:\n",
        "    factor = sh[2] // 256\n",
        "    img_sample = tf.reduce_mean(tf.reshape(img_sample, [-1, sh[1], sh[2] // factor, factor, sh[2] // factor, factor]), axis=[3,5])\n",
        "  return img_sample\n",
        "\n",
        "def embedding_function(image):\n",
        "  img_clone = downsample(image)\n",
        "  print(image.shape,img_clone.shape)\n",
        "  perceptual = VGG16_perceptual()\n",
        "  img_clone = tf.make_ndarray(img_clone)\n",
        "  img_clone = torch.from_numpy(img_clone)\n",
        "  syn0, syn1, syn2, syn3 = perceptual(img_clone)\n",
        "  print(syn0)\n",
        "\n",
        "embedding_function(img)\n",
        "\n",
        "w = np.zeros((1, 18, *Gs.input_shape[1:]))\n",
        "img = tf.cast(Gs.components.synthesis.get_output_for(w),tf.float32)\n",
        "img1 = tflib.convert_images_to_uint8(img, nchw_to_nhwc=True)\n",
        "print(img)\n",
        "\n",
        "proc_images_expr = (img + 1) * (255 / 2)\n",
        "sh = proc_images_expr.shape.as_list()\n",
        "if sh[2] > 256:\n",
        "  factor = sh[2] // 256\n",
        "  proc_images_expr = tf.reduce_mean(tf.reshape(proc_images_expr, [-1, sh[1], sh[2] // factor, factor, sh[2] // factor, factor]), axis=[3,5])\n",
        "print(proc_images_expr.shape)\n",
        "\n",
        "rnd = np.random.RandomState(7)\n",
        "z = rnd.randn(1, *Gs.input_shape[1:]) # [minibatch, component]\n",
        "z = np.zeros((1,512))\n",
        "print(z.shape)\n",
        "#tflib.set_vars({var: rnd.randn(*var.shape.as_list()) for var in noise_vars}) # [height, width]\n",
        "Gs_kwargs = {\n",
        "        'output_transform': dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True),\n",
        "        'randomize_noise': False\n",
        "    }\n",
        "label = np.zeros([1] + Gs.input_shapes[1][1:])\n",
        "print(label.shape)\n",
        "images = Gs.run(z, label, **Gs_kwargs) # [minibatch, height, width, channel]\n",
        "\n",
        "PIL.Image.fromarray(images[0], 'RGB').save(f'out/seed{rnd}.png')\n",
        "\n",
        "w = np.zeros((1, 18, *Gs.input_shape[1:]))\n",
        "print(w.shape)\n",
        "images = Gs.components.synthesis.run(w, output_transform=dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True))\n",
        "PIL.Image.fromarray(images[0], 'RGB').save(f'out/w_zeros{rnd}.png')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}